{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe54113",
   "metadata": {},
   "source": [
    "# Basic Attention Operation\n",
    "\n",
    "As you've learned, attention allows a seq2seq decoder to use information from each encoder step instead of just the final encoder hidden state. In the attention operation, the encoder outputs are weighted based on the decoder hidden state, then combined into one context vector. This vector is then used as input to the decoder to predict the next output step.\n",
    "\n",
    "In this ungraded lab, you'll implement a basic attention operation as described in [Bhadanau, et al (2014)](https://arxiv.org/abs/1409.0473) using Numpy. I'll describe each of the steps which you will be coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf5833",
   "metadata": {},
   "source": [
    "### Function 1: `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf02217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this first, a bit of setup for the rest of the lab\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x, axis=0):\n",
    "    \"\"\" Calculate softmax function for an array x along specified axis\n",
    "    \n",
    "        axis=0 calculates softmax across rows which means each column sums to 1 \n",
    "        axis=1 calculates softmax across columns which means each row sums to 1\n",
    "    \"\"\"\n",
    "    # subtract the max for numerical stability\n",
    "    x = x - np.expand_dims(np.max(x, axis=axis), axis)\n",
    "    # calculate the softmax\n",
    "    return np.exp(x) / np.expand_dims(np.sum(np.exp(x), axis=axis), axis)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aad331",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "The code snippet provided is a Python function implementing the **Softmax function**, a fundamental concept frequently utilized in machine learning, particularly in classification problems.\n",
    "\n",
    "### Overview of Softmax Function\n",
    "\n",
    "The **Softmax function** is utilized to convert a vector of raw scores (or \"logits\") into probabilities. Given an input vector `[z_1, z_2, ..., z_n]`, the Softmax function, %\\sigma%, outputs a vector `[p_1, p_2, ..., p_n]` where each `p_i` is defined as:\n",
    "\n",
    "$$p_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "\n",
    "### Key Characteristics of the Function\n",
    "\n",
    "#### 1. **Numerical Stability**\n",
    "   \n",
    "In the implementation, the maximum value along the specified axis is subtracted from each element before applying the exponential function. This is a common practice to enhance numerical stability and prevent potential overflow/underflow issues.\n",
    "\n",
    "```python\n",
    "x = x - np.expand_dims(np.max(x, axis=axis), axis)\n",
    "```\n",
    "\n",
    "#### 2. **Flexibility with Axis**\n",
    "\n",
    "The function is designed to calculate the Softmax across different axes (`axis=0` for columns, and `axis=1` for rows), providing flexibility depending on the desired application.\n",
    "\n",
    "- When `axis=0`, it calculates the Softmax across columns, meaning each column's values are transformed into probabilities and all values in a column sum to 1.\n",
    "- When `axis=1`, it calculates the Softmax across rows, meaning each row's values are converted into probabilities, and all values in a row sum to 1.\n",
    "\n",
    "#### 3. **Calculation of Probabilities**\n",
    "\n",
    "The Softmax is calculated by:\n",
    "   \n",
    "- Taking the exponential of each element in the input array, $e^{x_i}$.\n",
    "- Dividing each element by the sum of the exponentials along the specified axis.\n",
    "\n",
    "```python\n",
    "return np.exp(x) / np.expand_dims(np.sum(np.exp(x), axis=axis), axis)\n",
    "```\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "Suppose we have a 2x3 matrix representing raw score outputs from a model, and we want to convert them into probabilities:\n",
    "\n",
    "```python\n",
    "z = np.array([[1.0, 2.0, 3.0], \n",
    "              [1.0, 2.0, 3.0]])\n",
    "```\n",
    "\n",
    "Calculating the Softmax across rows:\n",
    "\n",
    "```python\n",
    "softmax_probs = softmax(z, axis=1)\n",
    "```\n",
    "\n",
    "`softmax_probs` will be a matrix of the same shape, with each row containing probabilities derived from the respective original raw scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655ed1f",
   "metadata": {},
   "source": [
    "## 1: Calculating alignment scores\n",
    "\n",
    "The first step is to calculate the alignment scores. This is a measure of similarity between the decoder hidden state and each encoder hidden state. From the paper, this operation looks like\n",
    "\n",
    "$$\n",
    "\\large e_{ij} = v_a^\\top \\tanh{\\left(W_a s_{i-1} + U_a h_j\\right)}\n",
    "$$\n",
    "\n",
    "where $W_a \\in \\mathbb{R}^{n\\times m}$, $U_a \\in \\mathbb{R}^{n \\times m}$, and $v_a \\in \\mathbb{R}^m$\n",
    "are the weight matrices and **$n$ is the hidden state size**. In practice, this is implemented as a feedforward neural network with two layers, where **$m$ is the size of the layers in the alignment network**. It looks something like:\n",
    "\n",
    "![alignment model](./images/alignment_model.png)\n",
    "\n",
    "Here $h_j$ are the encoder hidden states for each input step $j$ and $s_{i - 1}$ is the decoder hidden state of the previous step. The first layer corresponds to $W_a$ and $U_a$, while the second layer corresponds to $v_a$.\n",
    "\n",
    "To implement this, first concatenate the encoder and decoder hidden states to produce an array with size $K \\times 2n$ where $K$ is the number of encoder states/steps. For this, use `np.concatenate` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)). Note that there is only one decoder state so you'll need to reshape it to successfully concatenate the arrays. The easiest way is to use `decoder_state.repeat` ([docs](https://numpy.org/doc/stable/reference/generated/numpy.repeat.html#numpy.repeat)) to match the hidden state array size.\n",
    "\n",
    "Then, apply the first layer as a matrix multiplication between the weights and the concatenated input. Use the tanh function to get the activations. Finally, compute the matrix multiplication of the second layer weights and the activations. This returns the alignment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac415a56",
   "metadata": {},
   "source": [
    "### Function 2: `alignment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2e19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16 # size of hidden state of encoder and decoder\n",
    "''' \n",
    "attention_size(m) is set to 10. This means that the attention vectors (intermediate representations \n",
    "used to compute the alignment scores between encoder and decoder states) will be vectors of size 10. \n",
    "This is unrelated to the input length (5) or hidden size (16) and is a hyperparameter that could be tuned.\n",
    "'''\n",
    "attention_size = 10 # size of the attention \"vectors\" used to compute the alignment scores\n",
    "input_length = 5   # length of the input sequence\n",
    "\n",
    "np.random.seed(42) # set the random seed for reproducibility\n",
    "\n",
    "# Synthetic vectors used to test\n",
    "# h_j = encoder hidden states\n",
    "encoder_states = np.random.randn(input_length, hidden_size) # (5, 16)  5 words, each word has 16 features\n",
    "# s_i = decoder hidden state\n",
    "decoder_state = np.random.randn(1, hidden_size) # (1, 16) 1 word, 16 features\n",
    "\n",
    "# Synthetic weights used to test the alignment function\n",
    "# layer_1 2 hidden_size x attention size\n",
    "# This layer corresponds to W_a and U_a weights\n",
    "layer_1 = np.random.randn(2*hidden_size, attention_size) # (32, 10) 32 features, 10 attention vectors\n",
    "# layer_2 attention_size x 1  \n",
    "# This layer corresponds to v_a weights\n",
    "layer_2 = np.random.randn(attention_size, 1) # (10, 1) 10 attention vectors, 1 score\n",
    "\n",
    "# Implement this function. Replace None with your code. Solution at the bottom of the notebook\n",
    "def alignment(encoder_states, decoder_state):\n",
    "    ''' \n",
    "    - First, concatenate the encoder states and the decoder state along the appropriate axis\n",
    "    to produce an array with size Kx2n where K is the number of encoder states/steps aka number \n",
    "    of words and n is the hidden size. Then, multiply this concatenated array by layer_1 and apply \n",
    "    the tanh activation function. Finally, multiply the output of the previous layer by layer_2\n",
    "    and return the result.\n",
    "    \n",
    "    - np.tile(decoder_state, (input_length, 1))] repeats decoder_state 5 times to match encoder_states \n",
    "    shape. Note that decoder_state is (1, 16) and encoder_states is (5, 16). We can also use \n",
    "    np.repeat(decoder_state, input_length, axis=0) which gives the same result\n",
    "    '''\n",
    "    inputs = np.concatenate([encoder_states, np.tile(decoder_state, (input_length, 1))], axis=-1) # (5, 32) 5 words, 32 features\n",
    "    assert inputs.shape == (input_length, 2*hidden_size)  # assert inputs.shape == (5, 32)\n",
    "    \n",
    "    ''' \n",
    "    - Matrix multiplication of the concatenated inputs and layer_1, with tanh activation function\n",
    "    tanh function squashes the values between -1 and 1. Note that we can also use \n",
    "    np.matmul(inputs, layer_1) instead of np.dot(inputs, layer_1) which gives same results\n",
    "    '''\n",
    "    activations = np.tanh(np.dot(inputs, layer_1)) # (5, 10) 5 words, 10 attention vectors\n",
    "    assert activations.shape == (input_length, attention_size) # assert activations.shape == (5, 10)\n",
    "    \n",
    "    '''\n",
    "    Matrix multiplication of the activations with layer_2. \n",
    "    Remember that you don't need tanh here\n",
    "    '''\n",
    "    scores = np.dot(activations, layer_2) # (5, 1) 5 words, 1 score\n",
    "    assert scores.shape == (input_length, 1) # assert scores.shape == (5, 1)\n",
    "    \n",
    "    return scores # (5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0d0bc",
   "metadata": {},
   "source": [
    "### Explanation: \n",
    "\n",
    "Implementing Attention Mechanism for Sequence Alignment\n",
    "\n",
    "In the provided code snippet, we define and implement a simple attention mechanism for aligning encoder and decoder states in a sequence-to-sequence model. Below, let's dissect the various components and their responsibilities.\n",
    "\n",
    "```python\n",
    "hidden_size = 16 \n",
    "```\n",
    "This refers to the dimensionality of the hidden states in both the encoder and decoder of a sequence-to-sequence model, indicating that each state in the sequences is represented as a 16-dimensional vector.\n",
    "\n",
    "```python\n",
    "attention_size = 10 \n",
    "```\n",
    "Here, `attention_size` determines the size of attention vectors, which are intermediate representations used to calculate alignment scores between encoder and decoder states. This value is independent of the input length (5) or hidden size (16) and serves as a tunable hyperparameter.\n",
    "\n",
    "```python\n",
    "input_length = 5 \n",
    "```\n",
    "`input_length` designates the length of the input sequence which in this context refers to the number of words in the input sequence. \n",
    "\n",
    "Subsequently, synthetic data for encoder states and decoder states, and synthetic weights for attention alignment calculations are generated as follows:\n",
    "\n",
    "```python\n",
    "encoder_states = np.random.randn(input_length, hidden_size) \n",
    "decoder_state = np.random.randn(1, hidden_size) \n",
    "layer_1 = np.random.randn(2*hidden_size, attention_size) \n",
    "layer_2 = np.random.randn(attention_size, 1) \n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **encoder_states**: A synthetic set of encoder hidden states, each a 16-dimensional vector and 5 such vectors for each word in the input.\n",
    "- **decoder_state**: A single 16-dimensional vector representing the hidden state of the decoder.\n",
    "- **layer_1**: The weight matrix used to transform the concatenated encoder-decoder states into attention vectors. This correlates to the $W_a$ and $U_a$ weights in attention mechanism formulations.\n",
    "- **layer_2**: A second weight matrix that helps convert attention vectors into attention scores. This relates to the $v_a$ weight in attention mechanisms.\n",
    "\n",
    "**Defining the `alignment` function:**\n",
    "\n",
    "```python\n",
    "def alignment(encoder_states, decoder_state):\n",
    "```\n",
    "This function performs the following sequence of operations to calculate the attention scores:\n",
    "1. **Concatenation**: Merges each encoder state with the decoder state. For maintaining the shape consistency, the single decoder state is tiled to match the number of encoder states.\n",
    "2. **Attention Vector Calculation**: Performs a linear transformation on the concatenated states using `layer_1` weights and applies the tanh activation function.\n",
    "3. **Attention Score Calculation**: Derives the attention scores by multiplying the attention vectors with `layer_2` weights.\n",
    "\n",
    "**Attention Mechanism Logic:**\n",
    "- The concatenated encoder-decoder states are first transformed into attention vectors by multiplying them with a weight matrix (`layer_1`) and passing them through a non-linear tanh activation.\n",
    "- These attention vectors are then used to derive the attention scores by multiplying them with another weight matrix (`layer_2`). These scores represent how much attention the decoder state should pay to each encoder state during the decoding process.\n",
    "\n",
    "**General Notes:**\n",
    "- The attention mechanism allows the model to focus on different parts of the input sequence when producing each element of the output sequence, essentially enabling the model to have a \"memory\" of the input.\n",
    "- The choice of `attention_size`, `hidden_size`, and other hyperparameters should be motivated by both the empirical performance on validation data and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee06e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.35790943]\n",
      " [5.92373433]\n",
      " [4.18673175]\n",
      " [2.11437202]\n",
      " [0.95767155]]\n"
     ]
    }
   ],
   "source": [
    "# Run this to test your alignment function\n",
    "scores = alignment(encoder_states, decoder_state)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b964dcab",
   "metadata": {},
   "source": [
    "If you implemented the function correctly, you should get these scores:\n",
    "\n",
    "```python\n",
    "[[4.35790943]\n",
    " [5.92373433]\n",
    " [4.18673175]\n",
    " [2.11437202]\n",
    " [0.95767155]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719eeb9",
   "metadata": {},
   "source": [
    "## 2: Turning alignment into weights\n",
    "\n",
    "The next step is to calculate the weights from the alignment scores. These weights determine the encoder outputs that are the most important for the decoder output. These weights should be between 0 and 1, and add up to 1. You can use the softmax function (which I've already implemented above) to get these weights from the attention scores. Pass the attention scores vector to the softmax function to get the weights. Mathematically,\n",
    "\n",
    "$$\n",
    "\\large \\alpha_{ij} = \\frac{\\exp{\\left(e_{ij}\\right)}}{\\sum_{k=1}^K \\exp{\\left(e_{ik}\\right)}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 3: Weight the encoder output vectors and sum\n",
    "\n",
    "The weights tell you the importance of each input word with respect to the decoder state. In this step, you use the weights to modulate the magnitude of the encoder vectors. Words with little importance will be scaled down relative to important words. Multiply each encoder vector by its respective weight to get the alignment vectors, then sum up the weighted alignment vectors to get the context vector. Mathematically,\n",
    "\n",
    "$$\n",
    "\\large c_i = \\sum_{j=1}^K\\alpha_{ij} h_{j}\n",
    "$$\n",
    "\n",
    "Implement these steps in the `attention` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a131977",
   "metadata": {},
   "source": [
    "### Funtion 3: `attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be82b585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63514569  0.04917298 -0.43930867 -0.9268003   1.01903919 -0.43181409\n",
      "  0.13365099 -0.84746874 -0.37572203  0.18279832 -0.90452701  0.17872958\n",
      " -0.58015282 -0.58294027 -0.75457577  1.32985756]\n"
     ]
    }
   ],
   "source": [
    "# Attention function that takes in the encoder states and decoder state and returns the context vector \n",
    "def attention(encoder_states, decoder_state):\n",
    "    \"\"\" Example function that calculates attention, returns the context vector \n",
    "    \n",
    "        Arguments:\n",
    "        encoder_vectors: nxm numpy array, where n is the number of vectors and m is the vector length\n",
    "        decoder_vector: 1xm numpy array, m is the vector length, much be the same m as encoder_vectors\n",
    "    \"\"\" \n",
    "    \n",
    "    # First, calculate the alignment scores\n",
    "    scores = alignment(encoder_states, decoder_state) # (5, 1) 5 words, 1 score\n",
    "    \n",
    "    # Then take the softmax of the alignment scores to get a weight distribution\n",
    "    weights = softmax(scores, axis=0) # (5, 1) 5 words, 1 score values sum to 1\n",
    "    \n",
    "    # Multiply each encoder state by its respective weight\n",
    "    weighted_scores = np.multiply(encoder_states, weights) # (5, 16) 5 words, 16 features\n",
    "    \n",
    "    # Sum up weighted alignment vectors to get the context vector and return it\n",
    "    context = np.sum(weighted_scores, axis=0) # (16,) 16 features\n",
    "    return context\n",
    "\n",
    "context_vector = attention(encoder_states, decoder_state)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf3702",
   "metadata": {},
   "source": [
    "If you implemented the `attention` function correctly, the context vector should be\n",
    "\n",
    "```python\n",
    "[-0.63514569  0.04917298 -0.43930867 -0.9268003   1.01903919 -0.43181409\n",
    "  0.13365099 -0.84746874 -0.37572203  0.18279832 -0.90452701  0.17872958\n",
    " -0.58015282 -0.58294027 -0.75457577  1.32985756]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8a259",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The provided code snippet illustrates how an attention mechanism operates in the context of a sequence-to-sequence model in NLP. Specifically, the function `attention` computes a context vector based on the encoder states and a given decoder state. Let's break down the code and its methodology.\n",
    "\n",
    "```python\n",
    "def attention(encoder_states, decoder_state):\n",
    "    ...\n",
    "```\n",
    "This function takes in:\n",
    "- `encoder_states`: An $n \\times m$ array where $n$ denotes the number of encoder state vectors and $m$ is the dimensionality of each vector.\n",
    "- `decoder_state`: A $1 \\times m$ array representing the current state of the decoder.\n",
    "\n",
    "The function then proceeds to compute the context vector, which is crucial for determining the decoder's focus on the input sequence during the translation (or sequence generation) process.\n",
    "\n",
    "**Step 1: Calculate Alignment Scores**\n",
    "```python\n",
    "    scores = alignment(encoder_states, decoder_state) # (5, 1) 5 words, 1 score\n",
    "```\n",
    "The `alignment` function computes the *alignment scores* by determining how well each encoder state aligns with the current decoder state. A higher score implies a higher degree of alignment or attention.\n",
    "\n",
    "**Step 2: Compute Attention Weights**\n",
    "```python\n",
    "    weights = softmax(scores, axis=0) # (5, 1) 5 words, 1 score values sum to 1\n",
    "```\n",
    "Using the softmax function on the alignment scores across the first axis ensures that the scores are normalized and collectively sum to 1, thus forming a probability distribution. These normalized scores, now referred to as *attention weights*, designate how much focus the decoder should assign to each corresponding encoder state.\n",
    "\n",
    "**Step 3: Calculate Weighted Sum of Encoder States**\n",
    "```python\n",
    "    weighted_scores = np.multiply(encoder_states, weights) # (5, 16) 5 words, 16 features\n",
    "```\n",
    "Next, each encoder state is multiplied by its respective attention weight, creating a set of weighted encoder states. This step essentially scales each encoder state by the amount of attention it should receive from the decoder.\n",
    "\n",
    "**Step 4: Derive the Context Vector**\n",
    "```python\n",
    "    context = np.sum(weighted_scores, axis=0)\n",
    "    return context\n",
    "```\n",
    "Finally, the function computes the *context vector* by summing the weighted encoder states along the 0-axis (i.e., summing across all the weighted encoder states). This context vector, now representing a weighted sum of all encoder states, is utilized by the decoder to generate the next element in the output sequence, bearing in mind the relevant parts of the input sequence.\n",
    "\n",
    "This breakdown illustrates a typical implementation of attention mechanisms, ensuring that the model can allocate its focus adaptively across different parts of an input sequence when generating each word/token in the output. This is pivotal for handling long sequences and for tasks where different parts of the input are relevant at different stages of the output generation, like in machine translation, text summarization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d30df",
   "metadata": {},
   "source": [
    "### Operational Flow of the Seq2Seq Attention Model\n",
    "\n",
    "![operational flow of Seq2Seq Attention Model](./images/operational_flow_attention.png)\n",
    "\n",
    "This visualization details the step-by-step operations performed within this notebook for the sequence-to-sequence model with attention. From initial encoding of the input sequence to the final context-aware decoding, each stage of the process is meticulously illustrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42ba0f",
   "metadata": {},
   "source": [
    "## See below for solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Solution\n",
    "def alignment(encoder_states, decoder_state):\n",
    "    # First, concatenate the encoder states and the decoder state.\n",
    "    inputs = np.concatenate((encoder_states, decoder_state.repeat(input_length, axis=0)), axis=1)\n",
    "    assert inputs.shape == (input_length, 2*hidden_size)\n",
    "    \n",
    "    # Matrix multiplication of the concatenated inputs and the first layer, with tanh activation\n",
    "    activations = np.tanh(np.matmul(inputs, layer_1))\n",
    "    assert activations.shape == (input_length, attention_size)\n",
    "    \n",
    "    # Matrix multiplication of the activations with the second layer. Remember that you don't need tanh here\n",
    "    scores = np.matmul(activations, layer_2)\n",
    "    assert scores.shape == (input_length, 1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Run this to test your alignment function\n",
    "scores = alignment(encoder_states, decoder_state)\n",
    "print(scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Solution\n",
    "def attention(encoder_states, decoder_state):\n",
    "    \"\"\" Example function that calculates attention, returns the context vector \n",
    "    \n",
    "        Arguments:\n",
    "        encoder_vectors: NxM numpy array, where N is the number of vectors and M is the vector length\n",
    "        decoder_vector: 1xM numpy array, M is the vector length, much be the same M as encoder_vectors\n",
    "    \"\"\" \n",
    "    \n",
    "    # First, calculate the dot product of each encoder vector with the decoder vector\n",
    "    scores = alignment(encoder_states, decoder_state)\n",
    "    \n",
    "    # Then take the softmax of those scores to get a weight distribution\n",
    "    weights = softmax(scores)\n",
    "    \n",
    "    # Multiply each encoder state by its respective weight\n",
    "    weighted_scores = encoder_states * weights\n",
    "    \n",
    "    # Sum up the weights encoder states\n",
    "    context = np.sum(weighted_scores, axis=0)\n",
    "    \n",
    "    return context\n",
    "\n",
    "context_vector = attention(encoder_states, decoder_state)\n",
    "print(context_vector)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
